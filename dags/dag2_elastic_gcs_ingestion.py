"""
üöÄ DAG 2 : Kafka ‚Üí Elasticsearch + Google Cloud Storage

Ce DAG consomme les messages Kafka produits par le DAG 1,
les transforme, puis les envoie vers :
- Elasticsearch ‚Üí pour la visualisation sur Kibana
- GCS ‚Üí pour l‚Äôarchivage des donn√©es JSON

√âtapes :
1. ConsumeKafka ‚Üí lit un message du topic `result`
2. TransformJson ‚Üí nettoie et structure les donn√©es
3. PutElasticSearch ‚Üí indexe dans `taxi_rides`
4. PutGCP ‚Üí sauvegarde le JSON dans le bucket `laye2025`

Objectif : pipeline temps r√©el (Kafka ‚Üí Airflow ‚Üí Elastic + GCS)
"""



from __future__ import annotations
import json
import logging
from datetime import datetime
from airflow.decorators import dag, task
from kafka import KafkaConsumer
from airflow.providers.elasticsearch.hooks.elasticsearch import ElasticsearchHook
from airflow.providers.google.cloud.hooks.gcs import GCSHook
from elasticsearch import Elasticsearch  # ‚úÖ import en haut

log = logging.getLogger(__name__)

# -------------------------------------------------------------------
# üîß Configuration
# -------------------------------------------------------------------
KAFKA_BOOTSTRAP = "kafka:9092"
TOPIC_RESULT = "result"
ELASTIC_CONN_ID = "elasticsearch_default"
ELASTIC_INDEX = "taxi_rides"
GCP_CONN_ID = "google_cloud_default"
GCS_BUCKET = "laye2025"


# -------------------------------------------------------------------
# üß© Fonction utilitaire
# -------------------------------------------------------------------
def create_kafka_consumer(topic: str) -> KafkaConsumer:
    """Cr√©e un consommateur Kafka sans group_id (lit tout √† chaque run)."""
    return KafkaConsumer(
        topic,
        bootstrap_servers=[KAFKA_BOOTSTRAP],
        auto_offset_reset="earliest",
        enable_auto_commit=False,
        group_id=None,
        consumer_timeout_ms=10000,
    )


# -------------------------------------------------------------------
# üöÄ DAG Principal
# -------------------------------------------------------------------
@dag(
    dag_id="dag2_kafka_elastic_gcs",
    start_date=datetime(2025, 1, 1),
    schedule_interval="* * * * *",  # toutes les minutes
    catchup=False,
    tags=["kafka", "elasticsearch", "gcs", "big_data"],
)
def dag2_kafka_elastic_gcs():

    # 1Ô∏è‚É£ Consommer depuis Kafka
    @task(task_id="ConsumeKafka")
    def consume_kafka_result():
        log.info("üì• D√©marrage de la consommation du topic Kafka 'result'...")
        consumer = create_kafka_consumer(TOPIC_RESULT)
        message = next(consumer, None)
        consumer.close()

        if message is None:
            log.warning("‚ö†Ô∏è Aucun message re√ßu depuis Kafka.")
            return None

        msg_data = json.loads(message.value.decode("utf-8"))
        log.info(f"‚úÖ Message consomm√© depuis Kafka: {msg_data}")
        return msg_data

    # 2Ô∏è‚É£ Transformation JSON
    @task(task_id="TransformJson")
    def transform_for_elastic(data: dict | None):
        if not data:
            log.warning("Aucune donn√©e √† transformer.")
            return None

        transformed = {
            "confort": data.get("confort"),
            "prix_base_per_km": data.get("prix_base_per_km"),
            "client_nom": data.get("properties-client", {}).get("nomclient"),
            "driver_nom": data.get("properties-driver", {}).get("nomDriver"),
            "distance_km": data.get("distance_km"),
            "travel_cost": data.get("travel_cost"),
            "agent_timestamp": datetime.utcnow().isoformat(),
            "client_location": {
                "lat": data.get("properties-client", {}).get("latitude"),
                "lon": data.get("properties-client", {}).get("logitude"),
            },
            "driver_location": {
                "lat": data.get("properties-driver", {}).get("latitude"),
                "lon": data.get("properties-driver", {}).get("logitude"),
            },
        }
        log.info(f"üß© Donn√©es transform√©es: {transformed}")
        return transformed

    # 3Ô∏è‚É£ Envoi vers Elasticsearch
    @task(task_id="PutElasticSearch")
    def put_elasticsearch(data: dict | None):
        if not data:
            log.warning("Aucune donn√©e re√ßue pour Elasticsearch.")
            return

        es_hook = ElasticsearchHook(elasticsearch_conn_id=ELASTIC_CONN_ID)
        conn = es_hook.get_connection(ELASTIC_CONN_ID)

        # ‚úÖ Corrig√© : bloc bien indent√©
        es = Elasticsearch(
            hosts=[{"host": conn.host, "port": conn.port, "scheme": "http"}],
            basic_auth=(conn.login, conn.password) if conn.login else None,
            verify_certs=False
        )

        try:
            if not es.indices.exists(index=ELASTIC_INDEX):
                es.indices.create(index=ELASTIC_INDEX)
                log.info(f"‚úÖ Index '{ELASTIC_INDEX}' cr√©√©.")

            es.index(index=ELASTIC_INDEX, document=data)
            log.info(f"üì§ Donn√©e index√©e dans '{ELASTIC_INDEX}': {data}")

        except Exception as e:
            log.error(f"‚ùå Erreur d‚Äôinsertion Elasticsearch : {e}")

    # 4Ô∏è‚É£ Envoi vers Google Cloud Storage
    @task(task_id="PutGCP")
    def put_gcs(data: dict | None):
        if not data:
            log.warning("Aucune donn√©e re√ßue pour GCS.")
            return

        gcs_hook = GCSHook(gcp_conn_id=GCP_CONN_ID)
        file_name = f"rides/{datetime.utcnow().strftime('%Y%m%dT%H%M%S')}.json"
        json_data = json.dumps(data)

        gcs_hook.upload(bucket_name=GCS_BUCKET, object_name=file_name, data=json_data)
        log.info(f"‚úÖ Donn√©e sauvegard√©e sur GCS: gs://{GCS_BUCKET}/{file_name}")

    # -------------------------------------------------------------------
    # üîó Orchestration des t√¢ches
    # -------------------------------------------------------------------
    raw = consume_kafka_result()
    transformed = transform_for_elastic(raw)
    put_elasticsearch(transformed)
    put_gcs(transformed)


dag2_kafka_elastic_gcs()
